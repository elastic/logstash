[[ls-k8s-quick-start]]
== Quick start

WARNING: This documentation is still in development and may be changed or removed in a future release.

This guide describes how to set up {ls} to deliver Kubernetes logs to {es}. The logs will be monitored by Filebeat, processed through a Logstash pipeline, and then delivered into an {es} cluster in the Kubernetes environment.

This section includes the following topics:

* <<qs-prerequisites>>
* <<qs-set-up>>
* <<qs-generate-certificate>>
* <<qs-create-elastic-stack>>
* <<qs-view-monitoring-data>>
* <<qs-tidy-up>>
* <<qs-learn-more>>

[float]
[[qs-prerequisites]]
=== Prerequisites

Before you start, there are a few things you'll need:

. A running Kubernetes cluster - For single node testing we recommend using link:https://minikube.sigs.k8s.io[Minikube], which allows you to easily run a single node Kubernetes cluster on your system. Check the `Getting Started` section for install and set up instructions.
. A link:https://github.com/elastic/logstash/blob/feature/kubernetes/k8s/recipes/logstash-k8s-quickstart.zip[small zip file] of config files - Download and expand this archive into an empty directory on your local system. The files are described in <<sample-configuration-files,Sample configuration files>>.

[float]
[[qs-set-up]]
=== Set up your environment

.Setup the Elastic Stack
[%collapsible]
====


Let's start by getting your Minikube Kubernetes cluster up and running:

[source,sh]
--
minikube start
--

Install the Elastic custom resource definition (CRD) files, as well as the `elastic-operator` custom controller, which will be used to manage the Elastic resources in your cluster:

[source,sh]
--
kubectl create -f https://download.elastic.co/downloads/eck/2.4.0/crds.yaml
kubectl apply -f https://download.elastic.co/downloads/eck/2.4.0/operator.yaml
--

NOTE: The Elastic CRDs and ECK operator can also be set up using Elastic Helm charts, available at link:https://helm.elastic.co[https://helm.elastic.co].

Check the Kubernetes pods status to confirm that the `elastic-operator` pod is running:

[source,sh]
--
kubectl get pods
--

[source,sh]
--
NAME                 READY   STATUS    RESTARTS      AGE
elastic-operator-0   1/1     Running   4 (12m ago)   13d
--

[float]
[[qs-generate-certificate]]
=== Generate certificate files

To enable secure communication throughout your Kubernetes resources, run the sample script to generate the CA certificate files. Details about these files are in <<sample-configuration-files,Sample configuration files>>.

[source,sh]
--
./cert/generate_cert.sh
--

.**Expand to view output**
[%collapsible]
====
[source,sh]
--
Generating RSA private key, 2048 bit long modulus
.......................+++
...........................................................................+++
e is 65537 (0x10001)
Generating RSA private key, 2048 bit long modulus
..............................................+++
.............................................+++
e is 65537 (0x10001)
Signature ok
subject=/C=EU/ST=NA/O=Elastic/CN=ServerHostName
Getting CA Private Key
Generating RSA private key, 2048 bit long modulus
............+++
.......................................................................................................................................+++
e is 65537 (0x10001)
Signature ok
subject=/C=EU/ST=NA/O=Elastic/CN=ClientName
Getting CA Private Key
--
====

Your `logstash-k8s-gs/cert` folder should now contain a set of certificate files, including `client` certificates for Filebeat and Metricbeat, and `server` certificates for Logstash. 

The parent `logstash-k8s-gs` directory also has a new `001-secret.yaml` resources file that stores a hash of the client and server certificates.

image::./images/gs-cert-files.png[generated CA certificate files]

[float]
[[qs-create-elastic-stack]]
=== Create an Elastic Stack

Now that your environment and certificates are set up, it's time to create an Elastic Stack. Run the following command to deploy the example using the sample CRDs:



[source,sh]
--
kubectl apply -f "000-elasticsearch.yaml 005-filebeat.yaml 006-metricbeat.yaml 007-kibana.yaml"

--

The elastic stack resources are created:

[source,sh]
--
elasticsearch.elasticsearch.k8s.elastic.co/demo created
secret/logstash-beats-tls created
beat.beat.k8s.elastic.co/demo created
beat.beat.k8s.elastic.co/demo configured
kibana.kibana.k8s.elastic.co/demo created
--

[source,sh]
--
kubectl get pods
--

The pods are starting up. You may need to wait a minute or two for all of them to be ready.

[source,sh]
--
NAME                                    READY   STATUS    RESTARTS       AGE
demo-beat-filebeat-7f4d97f69f-qkkbl     1/1     Running   0              42s
demo-beat-metricbeat-59f4b68cc7-9zrrn   1/1     Running   0              39s
demo-es-default-0                       1/1     Running   0              41s
demo-kb-d7f585494-vbf6s                 1/1     Running   0              39s
elastic-operator-0                      1/1     Running   4 (164m ago)   13d
--

====

[float]
[[qs-set-up-logstash]]
==== Set up Logstash

Now we have an elastic stack to communicate with, let's set up Logstash.

Firstly, we'll create the `ConfigMaps` to hold logstash settings and
pipeline configuration. Each configuration file that we want to use, will require
its own `ConfigMap` definition. Note that rather than setting JVM settings in a
`jvm.options` config file, we recommend setting `LS_JAVA_OPTS`

[[qs-configmap]]
`001-configmap.yaml`::
This contains the Logstash settings and pipeline configuration:
+
[source,yaml]
--
  ---
# ConfigMap for logstash pipeline definition
data:
  logstash.conf: | <1>
    input {
      beats {
        port => "5044"
        ssl => true
        ssl_certificate_authorities => ["/usr/share/logstash/config/ca.crt"]
        ssl_certificate => "/usr/share/logstash/config/server.crt"
        ssl_key => "/usr/share/logstash/config/server.pkcs8.key"
        ssl_verify_mode => "force_peer"
      }
    }
    output {
      elasticsearch {
        hosts => ["https://demo-es-http:9200"]
        index => "kube-apiserver-%{+YYYY.MM.dd}"
        cacert => "/usr/share/logstash/config/es_ca.crt"
        user => 'elastic'
        password => '${ELASTICSEARCH_PASSWORD}'
      }
    }
---
# ConfigMap for logstash.yml definition
data:
  logstash.yml: | <2>
    api.http.host: "0.0.0.0"
--

<1> Definition of logstash configuration file. We will refer to this later in the deployment file, where we will define volumes
<2> Definition of `logstash.yml` file (link to logstash.yml file).
    Each key/value pair to override defaults will need to be defined here. This will be refered to later in
    the deployment file


Then, we'll create the deployment definition for {ls}, including memory, CPU resources, the container ports, timeout settings, and similar.
This file also includes the mount details for the secrets used in a secure setup:

[[qs-deployment]]
`002-deployment.yaml`::
Contains the configuration definition for {ls}.
+
[source,yaml]
--
spec:
  replicas: 1
  selector:
    matchLabels:
      app: logstash-demo
  template:
    metadata:
      labels:
        app: logstash-demo
    spec:
      containers:
        - name: logstash
          securityContext:
            runAsNonRoot: true
            runAsUser: 1000
          image: {docker-image} <1>
          env:
            - name: LS_JAVA_OPTS <2>
              value: "-Xmx1g -Xms1g"
            - name: ELASTICSEARCH_PASSWORD <11>
              valueFrom:
                secretKeyRef:
                  name: demo-es-elastic-user
                  key: elastic
          resources:
            limits: <3>
              cpu: 2000m
              memory: 2Gi
            requests:
              cpu: 1000m
              memory: 2Gi
          ports: <4>
            - containerPort: 9600
              name: stats
            - containerPort: 5044
              name: beats
          livenessProbe: <5>
            httpGet:
              path: /
              port: 9600
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          readinessProbe: <6>
            httpGet:
              path: /
              port: 9600
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          volumeMounts: <7>
            - name: logstash-pipeline
              mountPath: /usr/share/logstash/pipeline
            - name: logstash-config <8>
              mountPath: /usr/share/logstash/config/logstash.yml
              subPath: logstash.yml
            - name: es-certs <9>
              mountPath: /usr/share/logstash/config/es_ca.crt
              subPath: ca.crt
            - name: logstash-beats-tls
              mountPath: /usr/share/logstash/config/ca.crt
              subPath: ca.crt
            - name: logstash-beats-tls
              mountPath: /usr/share/logstash/config/server.pkcs8.key
              subPath: server.pkcs8.key
            - name: logstash-beats-tls
              mountPath: /usr/share/logstash/config/server.crt
              subPath: server.crt
      volumes:
        - name: logstash-pipeline <7>
          configMap:
            name: logstash-pipeline
        - name: logstash-config <8>
          configMap:
            name: logstash-config
        - name: es-certs <9>
          secret:
            secretName: demo-es-http-certs-public
        - name: logstash-beats-tls <10>
          secret:
            secretName: logstash-beats-tls
        - name: es-user <11>
          secret:
            secretName: demo-es-elastic-user
--

<1> Logstash docker image
<2> If you want to change any JVM settings from the default - most commonly setting memory. allocations - prefer to set them in the `LS_JAVA_OPTS` environment variable than recreating the whole `jvm.options` file in a `ConfigMap`.
<3> Resource/memory limits for the pod. Refer to Kubernetes documentation to set resources appropriately for each pod, but ensure that each pod has sufficient memory to handle the
    heap specified in <2>, allowing enough memory to deal with direct memory, see {logstash-ref}/config-details.html#heap-size[Logstash JVM settings] for details
<4> Expose the necessary ports on the container. Here are exposing port `5044` for the beats input, and `9600` for the metricbeat instance to query the logstash metrics API for stack monitoring purposes
<5> Liveness probe to determine whether Logstash is running. Here we point to the Logstash Metrics API, an HTTP based API that will be ready shortly after logstash starts. Note that the endpoint shows no indication that Logstash is active, only that the API is available
<6> Readiness probe to determine whether Logstash is running. Here we point to the Logstash Metrics API, an HTTP based API that will be ready shortly after logstash starts. Note that the endpoint shows no indication that Logstash is active, only that the API is available
<7> The pipeline configuration that we created in <<qs-configmap,the ConfigMap declaration>> needs a `volume` and a `volumeMount`. The `volume` refers to the created <<qs-configmap,config map>> and the `volumeMount` refers to the created `volume` and mounts in a location that logstash will read. Unless a separate `pipeline.yml` file is created by a further `ConfigMap` definition, the expected location of pipeline configurations is `/usr/share/logstash/pipelines` and the `mountPath` should be set accordingly.
<8> This refers to the <<qs-configmap,Logstash configuration>> we created earlier. This should consist of key/value pairs of entries overriding the default values in {logstash-ref}/logstash-settings-file.html[logstash.yml], using the `flat key syntax` described in that document. To setup, this needs a `volume` and a `volumeMount`. The `volume` refers to the created <<qs-configmap,config map>> and the `volumeMount` refers to the created `volume` and mounts in a location that logstash will read. The `mountPath` should be set to ` `/usr/share/logstash/logstash.yml`
<9> `Volume` and `VolumeMount` definitions for certificates to use with Elasticsearch. This contains the CA certificate to output data to {es}. Refer to link:https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-tls-certificates.html[TLS certificates] in the {eck} Guide for details.
<10> `Volume` and `VolumeMount` definitions for certificates to use with Beats
<11> The {es} password is taken from `demo-es-elastic-user` and passed to the Logstash pipeline as an `ELASTICSEARCH_PASSWORD` environment variable. Refer to link:https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-request-elasticsearch-endpoint.html[Access the {es} endpoint] in the {eck} Guide for details.


[[qs-service]]
`003-service.yaml`::
This file contains the Service definition, opening up ports on the logstash pods to the internal metricbeat (for stack monitoring) and filebeat in this instance

[source,yaml]
--
spec:
  type: ClusterIP
  ports:
    - port: 9600 <1>
      name: "stats"
      protocol: TCP
      targetPort: 9600 <1>
    - port: 5044 <2>
      name: "beats"
      protocol: TCP
      targetPort: 5044 <2>
  selector:
    app: logstash-demo
--

<1> Opening up port `9600` for metricbeat to connect to the Logstash metrics API
<2> Opening up port `5044` for filebeat to connect to the beats input defined in the <<qs-configmap,ConfigMap>>


[source,sh]
--
kubectl apply -f "001-secret.yaml,001-configmap.yaml,002-deployment.yaml,003-service.yaml"
--

We should now see the Logstash pod up and running

[source,sh]
--
kubectl get pods
--

The pods are starting up. You may need to wait a minute or two for all of them to be ready.

[source,sh]
--
NAME                                    READY   STATUS    RESTARTS       AGE
demo-beat-filebeat-7f4d97f69f-qkkbl     1/1     Running   0              42s
demo-beat-metricbeat-59f4b68cc7-9zrrn   1/1     Running   0              39s
demo-es-default-0                       1/1     Running   0              41s
demo-kb-d7f585494-vbf6s                 1/1     Running   0              39s
elastic-operator-0                      1/1     Running   4 (164m ago)   13d
logstash-7974b9ccb9-jd5xl               1/1     Running   0              42s
--


//Commenting out - not sure if we need this here, as it may confuse matters,
//we can talk about autoscaling in a subsequent section
//
//`004-hpa.yaml`::
//The Horizontal Pod Autoscaler is used to configure the horizontal scaling details for CPU and memory for the {ls} instance.


[float]
[[qs-view-monitoring-data]]
=== View the stack monitoring data

Now that your stack monitoring data is flowing, let's access it in {kib}. 

First, enable port forwarding for the {kib} service on port `5601`. Open a second shell window and run the following:

[source,sh]
--
kubectl port-forward service/demo-kb-http 5601
--

Then, open up a web browser at address `https://localhost:5601`. Depending on your browser you may need to accept the site certificate.

Log in to {kib} using the `elastic` username and password. To obtain the password, run:

[source,sh]
--
kubectl get secret demo-es-elastic-user -o=jsonpath='{.data.elastic}' | base64 --decode; echo
--

Open the {kib} main menu and select **Management**, then **Stack Monitoring**.

Select the {ls} **Overview**, and under the **Nodes** tab select the link for the {ls} node.

image::./images/gs-logstash-node-metrics.png[{ls} metrics data in {kib}]

That's it! The Kubernetes API server metrics data is flowing through {ls} into {es} and {kib}. You can monitor the JVM Heap, CPU Utilization, and System Load data as it updates in real time.

[float]
[[qs-tidy-up]]
=== Tidy up

After finishing with this demo, you can run the following command to remove all of the created resources:
 
[source,sh]
--
kubectl delete service,pods,deployment,hpa,configmap,secret,beat,elasticsearch,kibana -l app=logstash-demo
--

[float]
[[qs-learn-more]]
=== Learn more

Now that you're familiar with how to get a {ls} monitoring setup running in your Kubernetes environment, here are a few suggested next steps:

* <<ls-k8s-external-resource>>
* <<ls-k8s-design-for-plugins>>
* <<ls-k8s-sizing>>
* <<ls-k8s-secure>>
* <<ls-k8s-stack-monitoring>>

As well, we have a variety of <<ls-k8s-recipes,recipes>> that you can use as templates to configure an environment to match your specific use case.
