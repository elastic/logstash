[[ls-k8s-quick-start]]
== Quick start

WARNING: This documentation is still in development and may be changed or removed in a future release.

This guide walks you through setting up {ls} to deliver {k8s} logs to {es}. 
Tasks include setting up a {k8s} cluster that contains {es} and {kib} to store and visualize the logs.  
The logs are monitored by {filebeat}, processed through a {ls} pipeline, and then delivered to the {es} pod in the {k8s} cluster. 
We also walk you through configuring local stack monitoring using a {metricbeat} pod to monitor {ls}.

This section includes the following topics:

* <<qs-prerequisites>>
* <<qs-set-up>>
* <<qs-generate-certificate>>
* <<qs-create-elastic-stack>>
* <<qs-view-monitoring-data>>
* <<qs-tidy-up>>
* <<qs-external-elasticsearch>>
* <<qs-learn-more>>

[float]
[[qs-prerequisites]]
=== Prerequisites

You'll need:

* *A running {k8s} cluster.*  For local/single node testing we recommend using https://minikube.sigs.k8s.io[Minikube], which allows you to easily run a single node {k8s} cluster on your system. 
Check the minikube https://minikube.sigs.k8s.io/docs/start/[Get Started!] section for install and set up instructions.
* *A link:https://github.com/elastic/logstash/blob/main/docsk8s/sample-files/logstash-k8s-qs.zip[small zip file] of config files.* Download and expand this archive into an empty directory on your local system. The files are described in <<sample-configuration-files,Sample configuration files>>.

[float]
[[qs-set-up]]
=== Prepare your environment

[discrete]
[[qs-crd]]
==== Install Elastic CRDs

To simplify installing other elements of the {stack}, we will install Elastic custom resource definition (CRD) files and the `elastic-operator` custom controller, used to manage the Elastic resources in your cluster:


[source,sh]
--
kubectl create -f https://download.elastic.co/downloads/eck/2.4.0/crds.yaml
kubectl apply -f https://download.elastic.co/downloads/eck/2.4.0/operator.yaml
--

NOTE: The Elastic CRDs and ECK operator can also be set up using Elastic Helm charts, available at link:https://helm.elastic.co[https://helm.elastic.co].

Check the Kubernetes pods status to confirm that the `elastic-operator` pod is running:


[source,sh]
--
kubectl get pods
--

[source,sh]
--
NAME                 READY   STATUS    RESTARTS      AGE
elastic-operator-0   1/1     Running   4 (12m ago)   13d
--

[float]
[[qs-generate-certificate]]
==== Generate certificate files and create Kubernetes Secret definition

To help you enable secure communication between the {stack} components in your {k8s} cluster, we have provided a sample script to generate the CA certificate files. Details about these files are in <<sample-configuration-files,Sample configuration files>>.

[source,sh]
--
./cert/generate_cert.sh
--

.**Expand to view output**
[%collapsible]
====
[source,sh]
--
Generating RSA private key, 2048 bit long modulus
.......................+++
...........................................................................+++
e is 65537 (0x10001)
Generating RSA private key, 2048 bit long modulus
..............................................+++
.............................................+++
e is 65537 (0x10001)
Signature ok
subject=/C=EU/ST=NA/O=Elastic/CN=ServerHostName
Getting CA Private Key
Generating RSA private key, 2048 bit long modulus
............+++
.......................................................................................................................................+++
e is 65537 (0x10001)
Signature ok
subject=/C=EU/ST=NA/O=Elastic/CN=ClientName
Getting CA Private Key
--

Your `logstash-k8s-qs/cert` folder should now contain a set of certificate files, including `client` certificates for {filebeat} and {metricbeat}, and `server` certificates for {ls}.

The parent `logstash-k8s-qs` directory also has a new `001-secret.yaml` resources file that stores a hash of the client and server certificates.

image::./images/gs-cert-files.png[generated CA certificate files]

====

[float]
[[qs-create-kubernetes-cluster]]
=== Create the {k8s} cluster

As part of this configuration, we will set up {stack} components and {ls}.

[float]
[[qs-create-elastic-stack]]
==== Create the {stack} components

Now that your environment and certificates are set up, it's time to add the {stack}. We will create:

* {es} - you know, for search
* {kib} - for data visualization
* {filebeat} - to monitor container logs
* {metricbeat} - to monitor {ls} and send stack monitoring data to the monitoring cluster.
* Secret definitions containing the keys and certificates we generated earlier.

Run this command to deploy the example using the sample resources provided:

[source,sh]
--
kubectl apply -f "000-elasticsearch.yaml,001-secret.yaml,005-filebeat.yaml,006-metricbeat.yaml,007-kibana.yaml"
--

The {stack} resources are created:

[source,sh]
--
elasticsearch.elasticsearch.k8s.elastic.co/demo created
secret/logstash-beats-tls created
beat.beat.k8s.elastic.co/demo created
beat.beat.k8s.elastic.co/demo configured
kibana.kibana.k8s.elastic.co/demo created
--

[source,sh]
--
kubectl get pods
--

The pods are starting up. You may need to wait a minute or two for all of them to be ready.

[source,sh]
--
NAME                                    READY   STATUS    RESTARTS       AGE
demo-beat-filebeat-7f4d97f69f-qkkbl     1/1     Running   0              42s
demo-beat-metricbeat-59f4b68cc7-9zrrn   1/1     Running   0              39s
demo-es-default-0                       1/1     Running   0              41s
demo-kb-d7f585494-vbf6s                 1/1     Running   0              39s
elastic-operator-0                      1/1     Running   4 (164m ago)   13d
--


[float]
[[qs-set-up-logstash]]
==== Set up {ls}

We have our {stack} set up. Let's set up {ls}.

We typically use <<qs-configmap, ConfigMaps>> to set up {ls} configurations and pipeline definitions in {k8s}. 
Check out <<ls-k8s-configuration-files, Logstash Configuration files in Kubernetes>> for more details.


Then, we'll create the <<qs-deployment, deployment definition>> for {ls}, including memory, CPU resources, the container ports, timeout settings, and similar, and the <<qs-service, Service definition>>, opening up ports on the logstash pods to the internal metricbeat (for stack monitoring) and filebeat in this instance

Let's create a `Deployment`. 
Some {ls} configurations--such as those using certain classes of plugins or a persistent queue--should be configured using a `StatefulSet`.

[source,sh]
--
kubectl apply -f "001-configmap.yaml,002-deployment.yaml,003-service.yaml"
--

We should now see the Logstash pod up and running:

[source,sh]
--
kubectl get pods
--

The pods are starting up. You may need to wait a minute or two for all of them to be ready.

[source,sh]
--
NAME                                    READY   STATUS    RESTARTS       AGE
demo-beat-filebeat-7f4d97f69f-qkkbl     1/1     Running   0              42s
demo-beat-metricbeat-59f4b68cc7-9zrrn   1/1     Running   0              39s
demo-es-default-0                       1/1     Running   0              41s
demo-kb-d7f585494-vbf6s                 1/1     Running   0              39s
elastic-operator-0                      1/1     Running   4 (164m ago)   13d
logstash-7974b9ccb9-jd5xl               1/1     Running   0              42s
--



[float]
[[qs-view-data]]
=== View your data

First, enable port forwarding for the {kib} service on port `5601`. Open a second shell window and run:

[source,sh]
--
kubectl port-forward service/demo-kb-http 5601
--

Then, open up a web browser at address `https://localhost:5601`. Depending on your browser you may need to accept the site certificate.

Log in to {kib} using the `elastic` username and password. To obtain the password, run:

[source,sh]
--
kubectl get secret demo-es-elastic-user -o=jsonpath='{.data.elastic}' | base64 --decode; echo
--

We are sending two types of data to {es}: [k8s} logs and stack monitoring data.

[float]
[[qs-view-k8s-logs]]
==== View your {k8s} logs

The {filebeat} instance attached to this cluster sends log entries from the `kube-api-server` logs to an index specified in the {ls} configuration.

To verify that this data is indeed being sent to {es}, open the {kib} main menu and select **Management > Dev Tools**, and perform this query:

[source,http request]
--
GET kube-apiserver-*/_count
--

The count rises as events are discovered from the apiserver logs.

[source,json]
--
{
  "count": 89,
  "_shards": {
    "total": 1,
    "successful": 1,
    "skipped": 0,
    "failed": 0
  }
}
--



[float]
[[qs-view-monitoring-data]]
==== View the stack monitoring data

Open the {kib} main menu and select **Management**, then **Stack Monitoring**.

Select the {ls} **Overview**, and under the **Nodes** tab select the link for the {ls} node.

image::./images/gs-logstash-node-metrics.png[{ls} metrics data in {kib}]

That's it! The Logstash pod metrics data is flowing through {ls} into {es} and {kib}. You can monitor the JVM Heap, CPU Utilization, and System Load data as it updates in real time.

[float]
[[qs-tidy-up]]
=== Tidy up

After finishing with this demo, you can run the following command to remove all of the created resources:
 
[source,sh]
--
kubectl delete service,pods,deployment,configmap,secret,beat,elasticsearch,kibana -l app=logstash-demo
--


[float]
[[qs-next-steps]]
=== Next steps

[float]
[[qs-external-elasticsearch]]
==== Send logs to an external {es} instance


You aren't limited to sending data to an {es} cluster that is located in the same {k8s} cluster as {ls}. 
You can send data to Elastic cloud, for example.


[float]
[[qs-send-to-elastic-cloud]]
===== Sending to Elastic Cloud

We need only the {ls}-based components to connect to Elastic cloud. 
You won't need to include the {es} or {kib} components from the earlier examples.

Let's amend the `Deployment`/`StatefulSet` to set `CLOUD_ID` and `API_KEY` environment variables with the appropriate value for your cloud instance.

One way to do this is to create a link:https://kubernetes.io/docs/concepts/configuration/secret/[secret] to store `CLOUD_ID` and `API_KEY`:


[source,yaml]
--
apiVersion: v1
kind: Secret
metadata:
  name: ess_secret
type: Opaque
data:
  cloud_id: PENMT1VEX0lEPg== <1>
  password: PEFQSV9LRVk+
--
<1> base64 representation of `cloud_id` and `api_key` for your elastic cloud instance - created using:
+
[source,sh]
--
echo -n '<CLOUD_ID>' | base64
echo -n '<API_KEY>' | base64
--


Mount the secrets in the `Deployment`/`StatefulSet`:


[source,yaml]
--
env:
  - name: CLOUD_ID
      valueFrom:
        secretKeyRef:
          name: ess_secret
          key: cloud_id
    - name: API_KEY
      valueFrom:
        secretKeyRef:
          name: ess_secret
          key: api_key

--

Let's amend the pipeline definition `ConfigMap` to change the destination of the {es} output to the cloud instance.

[source,yaml]
--
    output {
      elasticsearch {
        cloud_id => "CLOUD_ID"
        api_key => "API_KEY"
        ssl => true
      }
--

[float]
[[qs-scale-logstash]]
==== Scale Logstash with Horizontal Pod Autoscaler

For a simple Logstash setup without <<ls-k8s-persistent-storage, persistent storage>> or <<ls-k8s-design-for-plugins, plugins that require the storing of local state>>, we can introduce a simple <<qs-autoscaler, horizontal pod autoscaler>>.

Apply the autoscaler:

[source,bash]
--
kubectl apply -f "004-hpa.yaml"
--

NOTE: If you are using more than one {ls} pod, use the https://www.elastic.co/guide/en/beats/metricbeat/current/configuration-autodiscover.html#_kubernetes[beats autodiscover] features to monitor them. Otherwise, only one {ls} pod is monitored.
See the <<monitor-with-ECK,stack monitoring with ECK>> docs for details on how to use autodiscover with {metricbeat} and {ls}.

[float]
[[qs-learn-more]]
==== Learn more

Now that you're familiar with how to get a {ls} monitoring setup running in your Kubernetes environment, here are a few suggested next steps:

* <<ls-k8s-design-for-plugins>>
* <<ls-k8s-sizing>>
* <<ls-k8s-secure>>
* <<ls-k8s-stack-monitoring>>

As well, we have a variety of <<ls-k8s-recipes,recipes>> that you can use as templates to configure an environment to match your specific use case.
