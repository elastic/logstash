[[ls-k8s-common-problems]]
=== Common problems

Following are some suggested resolutions to problems that you may encounter when running {ls} in a Kubernetes environment.

* <<problem-keep-restart>>
* <<problem-oom>>

[float]
[[problem-keep-restart]]
=== Logstash keeps restarting
When you check the running Kubernetes pods status, {ls} shows continual restarts. 

[source,bash]
--
NAMESPACE     NAME                                  READY   STATUS    RESTARTS      AGE
default       logstash-f7768c66d-grzbj              0/1     Running   3 (55s ago)   6m32s
--

This can be caused by a few issues:

[float]
[[problem-nometric]]
==== Metrics API not accessible to `readinessProbe`

If the `readinessProbe` is unable to access the health check endpoint, the {ls} process will be continuously stopped and restarted. To fix it, set the following in `logstash.yml` in `ConfigMap`.

[source,bash]
--
api.http.host: 0.0.0.0
--

[float]
[[problem-delay]]
==== {ls} startup process takes longer than `initialDelaySeconds`

Review the time constraints of `readinessProbe` and `livenessProbe` to ensure that {ls} has enough time to start up and expose the health check endpoint for the `readiness` and `liveness` probes to access.

[float]
[[problem-insufficient]]
==== Insufficient CPU or memory to start {ls}

Review CPU and memory usage using `kubectl top pods` (requires metrics server to be available for your Kubernetes implementation).

* Set the values of `cpu` and `memory` in your `Deployment` or `StatefulSet` appropriately.
* Ensure that the JVM memory settings are set appropriately. The default `Xmx` value is `1g`, and we recommend that heap size is set to no more than 50-75% of total memory.



[float]
[[problem-oom]]
=== {ls} stops with OOM errors
The status of {ls} shows `Ready,` but the pod repeatedly stops running.

This situation can be caused by insufficient memory. If {ls} uses more memory than the declared resource, Kubernetes shutdowns the pod immediately and the {ls} log does not show any shutdown related message.

Run `kubectl get event --watch` or `kubectl describe pod` if the event status shows `OOMKilled`.

The resolution is similar to the remedy for the insufficient CPU or memory problem. 
Review JVM and memory settings as shown in <<problem-insufficient>>.
