[[ea-integrations-tutorial]]
=== Tutorial: {ls} `elastic_integration filter` to extend Elastic {integrations}
++++
<titleabbrev>Tutorial: {ls} `elastic_integration filter`</titleabbrev>
++++

This tutorial walks you through configuring {ls} to transform events
collected by {agent} using our pre-built Elastic Integrations that normalize data to the Elastic Common Schema (ECS).
This is possible with a new feature in Logstash known as the elastic-integration
filter plugin.

When you include the elastic_integration filter in your configuration, {ls} reads certain field values generated by the {agent}, and uses them to apply the transformations from Elastic Integrations so that it can further process events before
sending them to their configured destinations.

[[ea-integrations-prereqs]]
*Prerequisites and requirements* 

You'll need:

* A working {es} cluster
* {fleet} server
* An {agent} configured to send output to {ls}
* An active Elastic Enterprise https://www.elastic.co/subscriptions[subscription]
* A user configured with the <<plugins-filters-elastic_integration-minimum_required_privileges,minimum required privileges>>

This feature can also be used with a self-managed agent, but the appropriate setup and configuration details
of using a self-managed agent aren't included in this guide.

[[ea-integrations-process-overview]]
*Process overview*

* <<ea-integrations-fleet>>
* <<ea-integrations-create-policy>>
* <<ea-integrations-pipeline>>

[discrete]
[[ea-integrations-fleet]]
=== Configure {fleet} to send data from {agent} to {ls}

. For {fleet} Managed Agent, go to {kib} and navigate to Fleet → Settings.
. Create a new output and specify {ls} as the output type.
. Add the {ls} hosts (domain or IP address/s) that the {agent} will send data to.
. Add the client SSL certificate and the Client SSL certificate key to the configuration.
    You can specify at the bottom of the settings if you would like to make this out the default for agent integrations. 
    By selecting this option, all Elastic Agent policies will default to using this Logstash output configuration.
. Click “Save and apply settings” in the bottom right-hand corner of the page.

[discrete]
[[ea-integrations-create-policy]]
=== Create an {agent} policy with the necessary integrations

. In {kib} navigate to Fleet → Agent policies and select “Create agent policy”.
. Give this policy a name, and then select “Advanced options”.
. Change the “Output for integrations” setting to the Logstash output you created in the last step.
. Click “Create agent policy” at the bottom of the flyout.
. The new policy should be listed on the Agent policies page now.
. Select the policy name so that we can start configuring an integration.
. On the policy page, click “Add integration”. 
  This will take you to the integrations browser, where you can select an integration that will have everything necessary to _integrate_ that data source with your other data in the Elastic stack.
. On the Crowdstrike integration overview page, click “Add Crowdstrike” to configure the integration.
. Configure the integration to collect the needed data.
On step 2 at the bottom of the page (Where to add this integration?), make sure the “Existing hosts” option
is selected and the Agent policy selected is our Logstash policy we created for our Logstash output. This
should be selected by default using the workflow of these instructions.
. Click “Save and continue” at the bottom of the page.
A modal will appear on the screen asking if you want to add the Elastic Agent to your hosts. If you have not
already done so, please install the Elastic Agent on a host somewhere. Documentation for this process can be
found here: https://www.elastic.co/guide/en/fleet/current/elastic-agent-installation.html


[discrete]
[[ea-integrations-pipeline]]
=== Configure {ls} to use the elastic_integration filter plugin

Create a new pipeline configuration in Logstash.

Every event sent from the Elastic Agent to Logstash contains specific meta-fields. 
Input event are expected to have data_stream.type, data_stream.dataset, and data_stream.namespace.
Logstash uses this information and its connection to Elasticsearch to determine which Integrations to apply to the event before sending that event to its destination output.
Logstash frequently synchronizes with Elasticsearch to ensure it has the most recent versions of the enabled Integrations.


All processing occurs in {ls}.


The user credentials that you specify in the elastic_integration plugin must have sufficient privileges to get information about Elasticsearch and the Integrations that are enabled.

Check out <<plugins-filters-elastic_integration-minimum_required_privileges>> for more info. 



**Sample configuration: output to {ess}**

This sample illustrates using the `elastic_agent` input, the `elastic_integration` filter for processing in {ls}, and sending the output to {ess}.

Check out the <<plugins-filters-elastic_integration,elastic_integration filter plugin docs>> for the full list of configuration options.




[source,txt]
-----
input {
 elastic_agent { port => 5055 }
}

filter {
 elastic_integration {
  cloud_id => "your-cloud:id"
  api_key => "api-key"
  remove_field => ["_version"]
 }
}

output {
 stdout {}
 elasticsearch {
  cloud_auth => "elastic:<pwd>"
  cloud_id => "your-cloud-id"
  }
 }
-----


**Sample configuration: output to self-managed {es}**

This sample illustrates using the `elastic_agent` input, the `elastic_integration` filter for processing in {ls}, and sending the output to self-managed {es}.

Check out the <<plugins-filters-elastic_integration,elastic_integration filter plugin docs>> for the full list of configuration options.



[source,txt]
-----
input {
 elastic_agent { port => 5055 }
}

filter {
 elastic_integration {
  hosts => "{es-host}:9200"
  ssl_enabled => true
  ssl_certificate_authorities => ["/usr/share/logstash/config/certs/ca-cert.pem"]
  auth_basic_username => "elastic"
  auth_basic_password => "changeme"
  remove_field => ["_version"]
 }
}

output {
 stdout {
  codec => rubydebug # to debug datastream inputs
 }
## add elasticsearch
 elasticsearch {
  hosts => "{es-host}:9200"
  password => "changeme"
  user => "elastic"
  cacert => "/usr/share/logstash/config/certs/ca-cert.pem"
 }
}
-----
